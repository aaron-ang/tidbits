{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Standard library imports\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from io import BytesIO\n",
    "from textwrap import wrap\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from xml.sax.saxutils import escape\n",
    "\n",
    "# Third-party library imports for asynchronous operations\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import contextlib\n",
    "\n",
    "\n",
    "# Data handling and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Moviepy for video editing and handling\n",
    "from moviepy.editor import (\n",
    "    AudioFileClip,\n",
    "    CompositeAudioClip,\n",
    "    CompositeVideoClip,\n",
    "    ColorClip,\n",
    "    ImageClip,\n",
    "    TextClip,\n",
    "    VideoFileClip,\n",
    "    concatenate_videoclips,\n",
    "    concatenate_audioclips,\n",
    ")\n",
    "\n",
    "# AI and Machine Learning APIs\n",
    "from openai import AsyncOpenAI\n",
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "# Additional utilities\n",
    "from suno import SongsGen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants:\n",
    "img_urls = [\n",
    "    \"\"\"https://cdn.discordapp.com/attachments/1212193794494042202/1223728760948523190/jas1.jpg?ex=661ae938&is=66087438&hm=a2661c174ebec9afba3265e3eb6bb86832f732f66d88195a5faa293a09839654&\"\"\",\n",
    "    \"\"\"https://cdn.discordapp.com/attachments/1221634008656642048/1223822939875704904/IMG_2084.jpg?ex=661b40ee&is=6608cbee&hm=ec453c6196ea2b6fc3bfa3e9fcb7f14a438f955f69198d937ea079da55a2afd6&\"\"\",\n",
    "]\n",
    "voice_ids = [\"jA3XNbtepbUtYOz5Q5bI\", \"UCHQN7CfyPSQTpOZoSvi\"]\n",
    "bearer = os.environ.get(\"DID_API_KEY\")\n",
    "encoded = base64.b64encode(bearer.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "eleven = os.environ.get(\"ELEVENLABS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\n\\n\n",
    "The above is a transcript of a lecture video with timestamps for each sentence in seconds.\n",
    "\n",
    "Your task is to create a 30 second engaging and educational tiktok script for one topic in the video. \n",
    "Choose one of the more obscure and interesting topics from the transcript that most people dont know about.\n",
    "The tiktok should incorporate an engaging story or example.\n",
    "Do not have any emojis or hashtags in the script.\n",
    "The script should be in ssml format. But do not change voices, only add pauses and emphasis.\n",
    "The script should sound passionate, excited, and happy.\n",
    "\"\"\"\n",
    "\n",
    "music_prompt = \"\"\"\n",
    "The above is a script for a tiktok video.\n",
    "Please generate a short one sentence description of the music that should be playing in the background of the video.\n",
    "Include genre and mood\n",
    "Example:\n",
    "\"A short upbeat EDM tune with a catchy melody\"\n",
    "\"\"\"\n",
    "\n",
    "transcript_prompt = \"\"\"\n",
    "You are given a transcript of a short video with timestamps\n",
    "You are in charge of making a list of pictures that will be used to create a video\n",
    "The video will be a slideshow of the pictures\n",
    "The pictures should be relevant to the text\n",
    "Make sure to include how long each picture should be displayed as well as the description of the picture\n",
    "\n",
    "Example output\n",
    "[{\"description\": \"A picture of a cat\", \"start\": 1, \"end\": 3}, {\"description\": \"A picture of a dog\", \"start\": 3, \"end\": 5}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_chunks(\n",
    "    input_file_path, binary_chunk_size=18 * 1024 * 1024\n",
    "):  # 18MB of binary data\n",
    "    chunks = []\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        suffix=\".mp3\", mode=\"wb\", delete=False\n",
    "    ) as temp_audio_file:\n",
    "        # Extract audio from MP4 or directly use MP3\n",
    "        if input_file_path.endswith(\".mp4\"):\n",
    "            video = AudioFileClip(input_file_path)\n",
    "            video.write_audiofile(temp_audio_file.name, codec=\"mp3\")\n",
    "        else:\n",
    "            with open(input_file_path, \"rb\") as audio_file:\n",
    "                temp_audio_file.write(audio_file.read())\n",
    "                temp_audio_file.flush()\n",
    "\n",
    "        temp_audio_file_name = temp_audio_file.name\n",
    "\n",
    "    # Reopen the temporary file in read-binary mode to read the audio data\n",
    "    with open(temp_audio_file_name, \"rb\") as temp_audio_file_rb:\n",
    "        audio_data = temp_audio_file_rb.read()\n",
    "\n",
    "    # Split the binary audio data into chunks and encode to Base64\n",
    "    for i in range(0, len(audio_data), binary_chunk_size):\n",
    "        chunk = audio_data[i : i + binary_chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Ensure to clean up the temporary file manually since delete=False\n",
    "    os.remove(temp_audio_file_name)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "async def transcribe_chunk(client, chunk, index):\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        suffix=\".mp3\", mode=\"wb\", delete=True\n",
    "    ) as temp_audio_file:\n",
    "        temp_audio_file.write(chunk)\n",
    "        temp_audio_file.flush()\n",
    "        temp_audio_file.seek(0)\n",
    "\n",
    "        with open(temp_audio_file.name, \"rb\") as audio_file:\n",
    "            transcript_obj = await client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", file=audio_file, response_format=\"verbose_json\"\n",
    "            )\n",
    "\n",
    "    # Include the chunk index to help with ordering and timestamp adjustments later\n",
    "    return index, transcript_obj\n",
    "\n",
    "\n",
    "async def transcribe_all_chunks(chunks):\n",
    "    client = AsyncOpenAI()\n",
    "    tasks = [transcribe_chunk(client, chunk, i) for i, chunk in enumerate(chunks)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Ensure the results are ordered by the original chunk index\n",
    "    ordered_results = sorted(results, key=lambda x: x[0])\n",
    "\n",
    "    return ordered_results\n",
    "\n",
    "\n",
    "def adjust_timestamps_and_combine(transcripts):\n",
    "    combined_transcript = []\n",
    "    total_duration = 0\n",
    "\n",
    "    for _, transcript_obj in transcripts:\n",
    "        segments = transcript_obj.segments\n",
    "        for segment in segments:\n",
    "            # Adjust timestamps\n",
    "            segment[\"start\"] += total_duration\n",
    "            segment[\"end\"] += total_duration\n",
    "            combined_transcript.append(\n",
    "                {\n",
    "                    \"text\": segment[\"text\"],\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Update total duration for the next chunk\n",
    "        last_segment = segments[-1]\n",
    "        total_duration = last_segment[\"end\"]\n",
    "\n",
    "    return combined_transcript\n",
    "\n",
    "\n",
    "async def transcribe_audio_chunks(chunks):\n",
    "    # Define the chunk size (18MB of decoded data is a safe estimate to stay under 25MB when encoded)\n",
    "    ordered_transcripts = await transcribe_all_chunks(chunks)\n",
    "    combined_transcript = adjust_timestamps_and_combine(ordered_transcripts)\n",
    "\n",
    "    return combined_transcript\n",
    "\n",
    "\n",
    "async def post_talk(script):\n",
    "    url = \"https://api.d-id.com/talks\"\n",
    "    # Randomly select one of the two voice IDs\n",
    "    randInt = np.random.randint(0, 2)\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": f\"Basic {encoded}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"x-api-key-external\": json.dumps({\"elevenlabs\": eleven}),\n",
    "    }\n",
    "    data = {\n",
    "        \"script\": {\n",
    "            \"type\": \"text\",\n",
    "            \"subtitles\": \"false\",\n",
    "            \"provider\": {\n",
    "                \"type\": \"elevenlabs\",\n",
    "                \"voice_id\": voice_ids[randInt],\n",
    "                \"voice_config\": {\"stability\": 0.3, \"similarity_boost\": 1},\n",
    "            },\n",
    "            \"model_id\": \"eleven_multilingual_v2\",\n",
    "            \"input\": script,\n",
    "            \"ssml\": True,\n",
    "        },\n",
    "        \"config\": {\"fluent\": \"false\", \"pad_audio\": \"0.0\"},\n",
    "        \"source_url\": img_urls[randInt],\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, headers=headers, json=data) as response:\n",
    "            # Check if the request was successful\n",
    "            if response.status == 201:\n",
    "                # Process the response here\n",
    "                response_data = await response.json()\n",
    "                id = response_data[\"id\"]\n",
    "\n",
    "            else:\n",
    "                # Handle request errors here\n",
    "                print(f\"Failed to post data, status code: {response.status}\")\n",
    "                print(await response.text())\n",
    "                return \"\"\n",
    "\n",
    "        # Loop to check the status\n",
    "        while True:\n",
    "            async with session.get(f\"{url}/{id}\", headers=headers) as status_response:\n",
    "                if status_response.status == 200:\n",
    "                    status_data = await status_response.json()\n",
    "                    if status_data[\"status\"] == \"done\":\n",
    "                        result_url = status_data[\"result_url\"]\n",
    "                        break\n",
    "                    else:\n",
    "                        # Wait for some time before checking the status again\n",
    "                        await asyncio.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to get data, status code: {status_response.status}\")\n",
    "                    return \"\"\n",
    "\n",
    "        # Download the video\n",
    "        async with session.get(result_url) as video_response:\n",
    "            if video_response.status == 200:\n",
    "                mp4_data = (\n",
    "                    await video_response.read()\n",
    "                )  # This is the binary data of the MP4 file\n",
    "                return mp4_data\n",
    "            else:\n",
    "                print(f\"Failed to download video, status code: {video_response.status}\")\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "async def async_generate_music(text):\n",
    "    i = SongsGen(os.environ.get(\"SUNO_COOKIE\"))\n",
    "    print(i.get_limit_left())\n",
    "    loop = asyncio.get_running_loop()\n",
    "\n",
    "    result = None\n",
    "    # Use a ThreadPoolExecutor to run synchronous functions in threads\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        result = await loop.run_in_executor(\n",
    "            pool,\n",
    "            lambda: i.get_songs(text, make_instrumental=True),\n",
    "        )\n",
    "\n",
    "    if not result:\n",
    "        return None\n",
    "\n",
    "    link = result[\"song_url\"]\n",
    "    print(\"Link: \", link)\n",
    "\n",
    "    attempt = 0\n",
    "    retry_delay = 5\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        while attempt < 5:\n",
    "            async with session.get(link) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.read()\n",
    "                    # Check if data is not empty\n",
    "                    if data:\n",
    "                        return data\n",
    "                    else:\n",
    "                        print(\"No data received, retrying in 5 seconds...\")\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Failed to fetch the song, status code: {response.status}, retrying in 5 seconds...\"\n",
    "                    )\n",
    "            await asyncio.sleep(retry_delay)  # Async sleep for retry_delay seconds\n",
    "            attempt += 1\n",
    "\n",
    "    print(\"Failed to fetch the song after retries\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Convert seconds to a time string in HH:MM:SS,MS format.\"\"\"\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},000\"\n",
    "\n",
    "\n",
    "async def transcribe_audio_file(audio_file_path):\n",
    "    client = AsyncOpenAI()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        transcript_obj = await client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\", file=audio_file, response_format=\"verbose_json\"\n",
    "        )\n",
    "    return transcript_obj\n",
    "\n",
    "\n",
    "async def fetch_image_binary(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            # Make sure the request was successful\n",
    "            if response.status == 200:\n",
    "                # Read and return the binary content of the image\n",
    "                return await response.read()\n",
    "            else:\n",
    "                # Handle possible HTTP errors (e.g., 404 Not Found) here if needed\n",
    "                return None\n",
    "\n",
    "\n",
    "async def generate_image(description):\n",
    "    client = AsyncOpenAI()\n",
    "    response = await client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=description,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "    image_binary = await fetch_image_binary(response.data[0].url)\n",
    "    return image_binary\n",
    "\n",
    "\n",
    "async def generate_images(pictures):\n",
    "    tasks = [generate_image(picture[\"description\"]) for picture in pictures]\n",
    "    imgs = await asyncio.gather(*tasks)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_video_and_generate_images(video_binary):\n",
    "    transcript = \"\"\n",
    "    srt_content = \"\"\n",
    "    # Save the video binary to a temporary file to process\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=True) as tmp_video_file:\n",
    "        tmp_video_file.write(video_binary)\n",
    "        transcript_obj = await transcribe_audio_file(tmp_video_file.name)\n",
    "        segments = transcript_obj.segments\n",
    "        for index, segment in enumerate(segments, start=1):\n",
    "            # Convert start and end times from seconds to the SRT time format\n",
    "            # Use the custom format_time function for start and end times\n",
    "            start_time = format_time(round(segment['start']))\n",
    "            end_time = format_time(round(segment['end']))\n",
    "\n",
    "            # Append the formatted segment to the SRT content string\n",
    "            srt_content += f\"{index}\\n{start_time} --> {end_time}\\n{segment['text']}\\n\\n\"\n",
    "            transcript += f\"{segment['text']} | start: {round(segment['start'],2)} | end: {round(segment['end'],2)}\\n\"\n",
    "    \n",
    "    async def generate_text(text, script):\n",
    "        client = AsyncAnthropic(\n",
    "            # This is the default and can be omitted\n",
    "            api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "        )\n",
    "\n",
    "        message = await client.messages.create(\n",
    "            temperature=0,\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text + script,\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": \"[\"},\n",
    "            ],\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "        )\n",
    "\n",
    "        return message.content\n",
    "\n",
    "    pictures = await generate_text(transcript, transcript_prompt)\n",
    "    pictures = json.loads(f\"[{pictures[0].text}\")\n",
    "   \n",
    "    images = []\n",
    "    images = await generate_images(pictures)\n",
    "    \n",
    "    return images, pictures, srt_content\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def parse_srt(srt_content):\n",
    "    \"\"\"Parse an SRT file into a list of dictionaries with 'start', 'end', and 'text'.\"\"\"\n",
    "\n",
    "    # Split into segments based on double line breaks\n",
    "    segments = re.split(r\"\\n\\n+\", srt_content)\n",
    "\n",
    "    subtitles = []\n",
    "    for segment in segments:\n",
    "        lines = segment.split(\"\\n\")\n",
    "        if len(lines) >= 3:\n",
    "            # Extract start and end times\n",
    "            times = re.findall(r\"(\\d{2}:\\d{2}:\\d{2},\\d{3})\", lines[1])\n",
    "\n",
    "            start_time = str_to_timedelta(times[0])\n",
    "            end_time = str_to_timedelta(times[1])\n",
    "\n",
    "            # The remaining lines are subtitle text\n",
    "            text = \"\\n\".join(lines[2:])\n",
    "\n",
    "            subtitles.append({\"start\": start_time, \"end\": end_time, \"text\": text})\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def str_to_timedelta(time_str):\n",
    "    \"\"\"Convert a time string from SRT format to a timedelta object.\"\"\"\n",
    "    datetime_obj = datetime.strptime(time_str, \"%H:%M:%S,%f\")\n",
    "    return timedelta(\n",
    "        hours=datetime_obj.hour,\n",
    "        minutes=datetime_obj.minute,\n",
    "        seconds=datetime_obj.second,\n",
    "        microseconds=datetime_obj.microsecond,\n",
    "    )\n",
    "\n",
    "\n",
    "def wrap_text(text, max_width):\n",
    "    # Wrap text to the specified width and join with newline to form up to 3 lines\n",
    "    return \"\\n\".join(wrap(text, max_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_mask(clip, radius=None):\n",
    "    \"\"\"\n",
    "    Applies a circular mask to the given clip, making the exterior of the circle transparent.\n",
    "    \"\"\"\n",
    "    if radius is None:\n",
    "        radius = min(clip.size) // 2\n",
    "\n",
    "    def mask_frame(frame):\n",
    "        h, w = frame.shape[:2]\n",
    "        Y, X = np.ogrid[:h, :w]\n",
    "        center = (h // 2, w // 2)\n",
    "        dist_from_center = np.sqrt((X - center[1]) ** 2 + (Y - center[0]) ** 2)\n",
    "\n",
    "        mask = dist_from_center <= radius\n",
    "        new_frame = frame.copy()\n",
    "        for i in range(3):  # Apply mask to each channel\n",
    "            new_frame[:, :, i] = frame[:, :, i] * mask\n",
    "\n",
    "        return new_frame\n",
    "\n",
    "    masked_clip = clip.fl_image(mask_frame)\n",
    "\n",
    "    # Create a mask clip\n",
    "    mask_clip = clip.fl_image(lambda frame: 255 * (mask_frame(frame) > 0))\n",
    "    masked_clip = masked_clip.set_mask(mask_clip.to_mask())\n",
    "\n",
    "    return masked_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_video(headshot, subtitles, pictures, images, music):\n",
    "    fade_duration = 0.5\n",
    "    image_clips = []\n",
    "\n",
    "    # Use contextlib.ExitStack to manage multiple context managers\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        # Create temporary files for headshot and music\n",
    "        tmp_headshot_file = stack.enter_context(\n",
    "            tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=True)\n",
    "        )\n",
    "        tmp_music_file = stack.enter_context(\n",
    "            tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=True)\n",
    "        )\n",
    "\n",
    "        # Write headshot and music binary data to their respective temporary files\n",
    "        tmp_headshot_file.write(headshot)\n",
    "        tmp_headshot_file.flush()  # Ensure data is written\n",
    "\n",
    "        tmp_music_file.write(music)\n",
    "        tmp_music_file.flush()  # Ensure data is written\n",
    "\n",
    "        # headshot is binary data of the headshot\n",
    "        headshot_clip = VideoFileClip(tmp_headshot_file.name).resize(height=500)\n",
    "        headshot_duration = headshot_clip.duration\n",
    "        headshot_audio = headshot_clip.audio\n",
    "\n",
    "        # Make audio slightly louder\n",
    "        headshot_audio = headshot_audio.volumex(1.5)\n",
    "\n",
    "        # Apply circular mask and position the headshot clip\n",
    "        headshot_clip = create_circular_mask(headshot_clip)\n",
    "        headshot_clip = headshot_clip.set_position((\"right\", \"bottom\")).margin(\n",
    "            right=50, bottom=50, opacity=0\n",
    "        )\n",
    "\n",
    "        for i, picture in enumerate(pictures):\n",
    "            img = Image.open(BytesIO(images[i]))\n",
    "            img_np = np.array(img)\n",
    "            img_clip = ImageClip(img_np)\n",
    "            # Resize the image to fit the width of the canvas\n",
    "            img_clip = img_clip.resize(width=1080)\n",
    "\n",
    "            # Create a black background clip with the same size as the canvas\n",
    "            black_bg = ColorClip(size=(1080, 1920), color=(0, 0, 0))\n",
    "\n",
    "            # Composite the image clip onto the black background clip\n",
    "            img_clip = CompositeVideoClip(\n",
    "                [black_bg, img_clip.set_position(\"center\")], size=(1080, 1920)\n",
    "            )\n",
    "\n",
    "            # Dynamically adjust the duration to extend to the start of the next picture, if applicable\n",
    "            if i < len(pictures) - 1:  # Check if there is a next picture\n",
    "                next_picture_start = pictures[i + 1][\"start\"]\n",
    "                img_clip_duration = next_picture_start - picture[\"start\"]\n",
    "            else:  # For the last picture, use its original end time\n",
    "                img_clip_duration = picture[\"end\"] - picture[\"start\"]\n",
    "            img_clip = img_clip.set_duration(img_clip_duration)\n",
    "            img_clip = img_clip.set_start(picture[\"start\"])\n",
    "\n",
    "            # Add fade-in effect to all but the first clip\n",
    "            if i > 0:\n",
    "                img_clip = img_clip.crossfadein(fade_duration)\n",
    "            # Add fade-out effect to all but the last clip\n",
    "            if i < len(pictures) - 1:\n",
    "                img_clip = img_clip.crossfadeout(fade_duration)\n",
    "\n",
    "            image_clips.append(img_clip)\n",
    "\n",
    "        # Concatenate image clips\n",
    "        video_clip = concatenate_videoclips(\n",
    "            image_clips, method=\"compose\", padding=-fade_duration\n",
    "        )\n",
    "\n",
    "        # Adjust the final image clip to match the headshot video's duration if necessary\n",
    "        if video_clip.duration < headshot_duration:\n",
    "            # Extend the last clip\n",
    "            last_clip = image_clips[-1].set_end(headshot_duration)\n",
    "            image_clips[-1] = last_clip\n",
    "            video_clip = concatenate_videoclips(image_clips, method=\"chain\")\n",
    "        elif video_clip.duration > headshot_duration:\n",
    "            # Truncate the video_clip to match the headshot_duration\n",
    "            video_clip = video_clip.subclip(0, headshot_duration)\n",
    "\n",
    "        # Processing subtitle clips\n",
    "        subtitle_clips = []\n",
    "        subtitles = parse_srt(subtitles)\n",
    "        for subtitle in subtitles:\n",
    "            # Create a TextClip for this subtitle\n",
    "            wrapped_text = wrap_text(subtitle[\"text\"], 40)\n",
    "            txt_clip = TextClip(\n",
    "                wrapped_text, fontsize=48, color=\"white\", font=\"Arial-Bold\", align=\"West\"\n",
    "            )\n",
    "\n",
    "            # Set the duration and start time for the TextClip\n",
    "            start_seconds = subtitle[\"start\"].total_seconds()\n",
    "            end_seconds = subtitle[\"end\"].total_seconds()\n",
    "            txt_clip = txt_clip.set_start(start_seconds).set_duration(\n",
    "                end_seconds - start_seconds\n",
    "            )\n",
    "\n",
    "            # Set the position of the TextClip in the top middle of the screen\n",
    "            txt_clip = txt_clip.set_position((\"center\", \"top\")).margin(top=50, opacity=0)\n",
    "\n",
    "            subtitle_clips.append(txt_clip)\n",
    "\n",
    "        background_music = AudioFileClip(tmp_music_file.name)\n",
    "        repeat_count = int(headshot_duration // background_music.duration) + 1\n",
    "        # Create a list with the audio clip repeated\n",
    "        repeated_clips = [background_music] * repeat_count\n",
    "\n",
    "        # Concatenate the repeated clips\n",
    "        looped_background_music = concatenate_audioclips(repeated_clips)\n",
    "\n",
    "        # Trim the concatenated audio to match the headshot_duration\n",
    "        looped_background_music = looped_background_music.subclip(0, headshot_duration)\n",
    "\n",
    "        final_audio = CompositeAudioClip([headshot_audio, looped_background_music])\n",
    "\n",
    "        # Create the final composite clip\n",
    "        final_clip = CompositeVideoClip(\n",
    "            [\n",
    "                video_clip.set_duration(headshot_duration),\n",
    "                *subtitle_clips,\n",
    "                headshot_clip.set_duration(headshot_duration),\n",
    "            ],\n",
    "            size=(1080, 1920),\n",
    "        ).set_audio(final_audio)\n",
    "\n",
    "        # Write the final video to a file\n",
    "        final_clip.write_videofile(\"data/final_video.mp4\", threads=8, fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /var/folders/9q/qp70wln55bd5fkdxdy0jw5t80000gn/T/tmp01p5g1b3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcription took:  26.578253269195557\n",
      "Script generation took:  29.341792821884155\n",
      "182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Waiting for results<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Waiting for results\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">.</pre>\n"
      ],
      "text/plain": [
       "."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Token expired, will sleep <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> seconds and try to download\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Token expired, will sleep \u001b[1;36m30\u001b[0m seconds and try to download\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link:  https://audiopipe.suno.ai/?item_id=1a9c39b9-5069-4da1-be92-0498c47e6277\n",
      "No data received, retrying in 5 seconds...\n",
      "No data received, retrying in 5 seconds...\n",
      "Music and headshot generation took:  172.02414298057556\n",
      "Moviepy - Building video data/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video data/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready data/final_video.mp4\n",
      "Video editing took:  128.78781914710999\n"
     ]
    }
   ],
   "source": [
    "async def generate(file_path):\n",
    "    input_file_path = file_path\n",
    "    client = AsyncAnthropic(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    # Initialize the data dictionary\n",
    "    script = None\n",
    "    music = None\n",
    "    images = None\n",
    "    pictures = None\n",
    "    captions = None\n",
    "\n",
    "    # This section transcribes the audio from the input file\n",
    "    cur_time = time.time()\n",
    "    chunks = convert_audio_to_chunks(input_file_path)\n",
    "    transcripts = await transcribe_audio_chunks(chunks)\n",
    "    text = \"\"\n",
    "    for transcript in transcripts:\n",
    "        text += \"(\" + str(transcript[\"start\"]) + \"): \" + transcript[\"text\"] + \" \"\n",
    "\n",
    "    print(\"Transcription took: \", time.time() - cur_time)\n",
    "\n",
    "    # This section uses the text to generate a script\n",
    "    cur_time = time.time()\n",
    "    async def generate_text(text, script):\n",
    "        message = await client.messages.create(\n",
    "            temperature=0,\n",
    "            max_tokens=512,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text + script,\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": \"<\"},\n",
    "            ],\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "        )\n",
    "\n",
    "        return message.content\n",
    "\n",
    "    response = await generate_text(text, prompt)\n",
    "    script = f\"<{response[0].text}\"\n",
    "\n",
    "    async def generate_text(text, script):\n",
    "        message = await client.messages.create(\n",
    "            temperature=0,\n",
    "            max_tokens=64,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text + script,\n",
    "                },\n",
    "            ],\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "        )\n",
    "\n",
    "        return message.content\n",
    "\n",
    "    response = await generate_text(script, music_prompt)\n",
    "    music = response[0].text\n",
    "    print(\"Script generation took: \", time.time() - cur_time)\n",
    "\n",
    "    async def generate_helper(script):\n",
    "        # Generating the talking head\n",
    "        headshot = await post_talk(script)\n",
    "        # Generates pictuers and captions for the video\n",
    "        images, pictures, captions = await process_video_and_generate_images(headshot)\n",
    "        return headshot, images, pictures, captions\n",
    "\n",
    "    cur_time = time.time()\n",
    "    music_result, headshot_result = await asyncio.gather(\n",
    "        async_generate_music(music),\n",
    "        generate_helper(script),\n",
    "    )\n",
    "    print(\"Music and headshot generation took: \", time.time() - cur_time)\n",
    "    music = music_result\n",
    "    headshot, images, pictures, captions = headshot_result\n",
    "\n",
    "    # Edit the video\n",
    "    cur_time = time.time()\n",
    "    edit_video(headshot, captions, pictures, images, music)\n",
    "    print(\"Video editing took: \", time.time() - cur_time)\n",
    "\n",
    "await generate(\"data/short_dsa.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidbits3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
